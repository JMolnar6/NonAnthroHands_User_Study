{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Time Warping for Non-Anthropomorphic Hand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up:\n",
    "# %matplotlib widget\n",
    "# %matplotlib inline\n",
    "# %matplotlib ipympl\n",
    "# %matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting Packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from ipywidgets import *\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "savefig_options = dict(format=\"png\", dpi=150, bbox_inches=\"tight\")\n",
    "\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# import ipywidgets as widget\n",
    "\n",
    "# Computation packages\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.signal import find_peaks\n",
    "from fastdtw import fastdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nah.alignments import load_raw_csv_data\n",
    "from nah.utils import norm_data, full_align, full_joint_align,clean_rot_data, load_raw_csv_data, load_npzs, segmentbydemo, sumofsquares\n",
    "\n",
    "from nah.plot import plot_norm, plot_pos, plot_rot, plot_raw_data, plot_raw_data_subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (1330, 7) (4,)\n",
      "2 (1330, 7) (4,)\n",
      "3 (1330, 7) (4,)\n",
      "4 (1330, 7) (4,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m demo_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     48\u001b[0m peaks, _ \u001b[38;5;241m=\u001b[39m find_peaks(total_end_eff[:, \u001b[38;5;241m0\u001b[39m], height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m end_eff, camera, rh, lh, joints \u001b[38;5;241m=\u001b[39m segmentbydemo(total_end_eff, total_camera, total_rh, total_lh, total_joint, demo_max, peaks)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     51\u001b[0m     plot_raw_data_subsampled(\u001b[38;5;241m5\u001b[39m, end_eff[i], camera[i], rh[i], lh[i], joint[i])\n",
      "File \u001b[1;32m~\\Documents\\Projects\\NonAnthroHands_User_Study\\python\\nah\\utils.py:202\u001b[0m, in \u001b[0;36msegmentbydemo\u001b[1;34m(end_eff_data, camera_data, rh_data, lh_data, joint_data, demo_max, peaks)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i, end_eff_data\u001b[38;5;241m.\u001b[39mshape, peaks\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 202\u001b[0m     end_eff[i] \u001b[38;5;241m=\u001b[39m end_eff_data[peaks[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]:peaks[i], :]\n\u001b[0;32m    203\u001b[0m     camera[i] \u001b[38;5;241m=\u001b[39m camera_data[peaks[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]:peaks[i], :]\n\u001b[0;32m    204\u001b[0m     rh[i] \u001b[38;5;241m=\u001b[39m rh_data[peaks[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]:peaks[i], :]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "# %matplotlib inline\n",
    "# %matplotlib ipympl\n",
    "# %matplotlib notebook\n",
    "\n",
    "# robot_name='j2s6s300'\n",
    "robot_name='Reachy'\n",
    "gesture_num=11\n",
    "\n",
    "\n",
    "total_end_eff = np.array([])\n",
    "total_camera  = np.array([])\n",
    "total_rh      = np.array([])\n",
    "total_lh      = np.array([])\n",
    "total_joint   = np.array([])\n",
    "\n",
    "singlePID=True\n",
    "singlePIDval = 12\n",
    "followup = False\n",
    "\n",
    "if singlePID:\n",
    "    PID_begin_range=singlePIDval\n",
    "    PID_end_range=singlePIDval+1 #Don't forget to +1 to whatever your last PID is\n",
    "else:\n",
    "    PID_begin_range=1\n",
    "    if followup:\n",
    "        PID_end_range=10 #Don't forget to +1 to whatever your last PID is\n",
    "    else:\n",
    "        PID_end_range=17\n",
    "for PID in range(PID_begin_range,PID_end_range):\n",
    "    end_eff, camera, rh, lh, joint = load_npzs(robot_name, PID, followup, gesture_num)\n",
    "    if (PID==PID_begin_range):\n",
    "        total_end_eff = end_eff\n",
    "        total_camera = camera\n",
    "        total_rh = rh\n",
    "        total_lh = lh\n",
    "        total_joint = joint\n",
    "    else:\n",
    "        total_end_eff = np.vstack((total_end_eff,end_eff))\n",
    "        total_camera  = np.vstack((total_camera,camera))\n",
    "        total_rh      = np.vstack((total_rh,rh))\n",
    "        total_lh      = np.vstack((total_lh,lh))\n",
    "        total_joint   = np.vstack((total_joint,joint))\n",
    "# plot_raw_data(end_eff, rh, lh, camera, joint, start_index, end_index)\n",
    "# plot_raw_data_subsampled(5, total_end_eff, total_camera, total_rh, total_lh, total_joint)\n",
    "\n",
    "demo_max=5\n",
    "end_eff, camera, rh, lh, joints = segment_by_demo(total_end_eff, total_camera, total_rh, total_lh, total_joint, demo_max)\n",
    "for i in range(0,5):\n",
    "    plot_raw_data_subsampled(5, end_eff[i], camera[i], rh[i], lh[i], joint[i])\n",
    "\n",
    "plot_raw_data_subsampled(5, total_end_eff, total_camera, total_rh, total_lh, total_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.core import metrics, sync\n",
    "from evo.core.trajectory import PoseTrajectory3D\n",
    "from scipy.spatial.transform import Rotation\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_evo_trajectory(traj):\n",
    "    \"\"\"timestamp, tx, ty, tz, rx, ry, rz\"\"\"\n",
    "    timestamps = traj[:, 0]\n",
    "    xyz = traj[:, 1:4]\n",
    "\n",
    "    euler_angles = traj[:, 4:7]\n",
    "    Rs = Rotation.from_euler('xyz', euler_angles)\n",
    "    quat_xyzw = Rs.as_quat()\n",
    "    quat_wxyz = quat_xyzw[:, (3, 0, 1, 2)]\n",
    "    \n",
    "    return PoseTrajectory3D(positions_xyz=xyz,\n",
    "                           orientations_quat_wxyz=quat_wxyz,\n",
    "                           timestamps=timestamps)\n",
    "\n",
    "def evo_sync(traj1: PoseTrajectory3D, traj2: PoseTrajectory3D):\n",
    "    \"\"\"Synchronize trajectories using Evo's associate_trajectories method\"\"\"\n",
    "    traj1, traj2 = sync.associate_trajectories(traj1, traj2)\n",
    "    return traj1, traj2\n",
    "    \n",
    "\n",
    "def align(traj1, traj2, correct_scale=True):\n",
    "    \"\"\"\n",
    "    Align the first trajectory to the second one.\n",
    "    Returns the aligned first trajectory.\n",
    "    \"\"\"\n",
    "    traj1_aligned = deepcopy(traj1)\n",
    "    r, t, s = traj1_aligned.align(traj2, correct_scale=correct_scale)\n",
    "    print(r, t, s)\n",
    "    return traj1_aligned\n",
    "    \n",
    "def evaluate_ape(traj1: PoseTrajectory3D, traj2: PoseTrajectory3D):\n",
    "    metric = metrics.APE(metrics.PoseRelation.full_transformation)\n",
    "    metric.process_data((traj1, traj2))\n",
    "    \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "end_eff_traj = get_evo_trajectory(end_eff[0])\n",
    "right_hand_traj = get_evo_trajectory(rh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "end_eff_traj, right_hand_traj = evo_sync(end_eff_traj, right_hand_traj)\n",
    "\n",
    "metric = evaluate_ape(end_eff_traj, right_hand_traj)\n",
    "print(metric.get_all_statistics())\n",
    "\n",
    "right_hand_traj = align(right_hand_traj, end_eff_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_evo_to_np(traj: PoseTrajectory3D, shape):\n",
    "    array = np.empty(shape)\n",
    "    array[:, 0] = traj.timestamps\n",
    "    array[:, 1:4] = traj.positions_xyz\n",
    "    array[:, 4:7] = traj.get_orientations_euler()\n",
    "    return array\n",
    "\n",
    "end_eff_aligned = convert_evo_to_np(end_eff_traj, end_eff[0].shape)\n",
    "print(end_eff_aligned.shape)\n",
    "\n",
    "rh_aligned = convert_evo_to_np(right_hand_traj, rh[0].shape)\n",
    "print(rh_aligned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_raw_data_subsampled(5, end_eff_aligned, camera[0], rh_aligned, lh[0], joint[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate_ape(end_eff_traj, right_hand_traj)\n",
    "metric.get_all_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evo_metrics(traj1, traj2):\n",
    "    traj1_evo = get_evo_trajectory(traj1)\n",
    "    traj2_evo = get_evo_trajectory(traj2)   \n",
    "    \n",
    "    traj1_evo, traj2_evo = evo_sync(traj1_evo, traj2_evo)\n",
    "    \n",
    "    metric = evaluate_ape(traj1_evo, traj2_evo)\n",
    "    return(metric.get_all_statistics())\n",
    "\n",
    "def get_aligned_evo_metrics(traj1, traj2):\n",
    "    traj1_evo = get_evo_trajectory(traj1)\n",
    "    traj2_evo = get_evo_trajectory(traj2)   \n",
    "    \n",
    "    traj1_evo, traj2_evo = evo_sync(traj1_evo, traj2_evo)    \n",
    "    # metric = evaluate_ape(traj1_evo, traj2_evo)\n",
    "    \n",
    "    traj2_evo = align(traj2_evo, traj1_evo)\n",
    "\n",
    "    traj1_aligned = convert_evo_to_np(traj1_evo, traj1.shape)\n",
    "    traj2_aligned = convert_evo_to_np(traj2_evo, traj2.shape)\n",
    "\n",
    "    metric = evaluate_ape(traj1_aligned, traj2_aligned)\n",
    "    return(metric.get_all_statistics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparison between RH and robot end-eff\n",
    "# (We'll want comparisons between the person's different demos, and the end-eff and the lh, also)\n",
    "\n",
    "# robot_name='j2s6s300'\n",
    "robot_name='Reachy'\n",
    "\n",
    "followup = False\n",
    "\n",
    "if followup:\n",
    "    PID_max = 9\n",
    "    gesture_max = 6\n",
    "else: \n",
    "    PID_max = 16\n",
    "    gesture_max = 15\n",
    "\n",
    "for singlePIDval in range(1,PID_max+1):\n",
    "    for gesture_num in range(1,gesture_max+1):\n",
    "        end_eff, camera, rh, lh, joint = load_npzs(robot_name, PID, followup, gesture_num)\n",
    "        #TODO Jennifer: ***ERROR*** total_joint is all zeros for Jaco right now!!! temp placeholder until we get JointMotion to save properly for the Jaco\n",
    "\n",
    "        demo_max=5\n",
    "        end_eff, camera, rh, lh, joints = segment_by_demo(total_end_eff, total_camera, total_rh, total_lh, total_joint, demo_max)\n",
    "        # for i in range(0,5):\n",
    "        #     plot_raw_data_subsampled(5, end_eff[i], camera[i], rh[i], lh[i], joint[i])\n",
    "\n",
    "        for demo_num in range(0,5):\n",
    "            end_eff_traj = get_evo_trajectory(end_eff[i])\n",
    "            right_hand_traj = get_evo_trajectory(rh[i])   \n",
    "\n",
    "            end_eff_traj, right_hand_traj = evo_sync(end_eff_traj, right_hand_traj)\n",
    "\n",
    "            metric = evaluate_ape(end_eff_traj, right_hand_traj)\n",
    "            print(metric.get_all_statistics()) # Not aligned statistics\n",
    "            \n",
    "            right_hand_traj = align(right_hand_traj, end_eff_traj)\n",
    "\n",
    "            end_eff_aligned = convert_evo_to_np(end_eff_traj, end_eff[i].shape)\n",
    "            # print(end_eff_aligned.shape)\n",
    "            \n",
    "            rh_aligned = convert_evo_to_np(right_hand_traj, rh[i].shape)\n",
    "            # print(rh_aligned.shape)\n",
    "\n",
    "            metric = evaluate_ape(end_eff_traj, right_hand_traj)\n",
    "            metric.get_all_statistics() #Aligned statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparison between RH demos\n",
    "# (We'll want comparisons between the person's different demos (both RH and LH, depending), and the end-eff and the hands, also)\n",
    "\n",
    "# robot_name='j2s6s300'\n",
    "robot_name='Reachy'\n",
    "\n",
    "followup = False\n",
    "demo_max = 5\n",
    "\n",
    "if followup:\n",
    "    PID_max = 9\n",
    "    gesture_max = 6\n",
    "else: \n",
    "    PID_max = 16\n",
    "    gesture_max = 15\n",
    "\n",
    "\n",
    "for singlePIDval in range(1,PID_max+1):\n",
    "# for singlePIDval in range(1,2):    \n",
    "    for gesture_num in range(1,gesture_max+1):\n",
    "    # for gesture_num in range(1,2):\n",
    "        end_eff, camera, rh, lh, joint = load_npzs(robot_name, PID, followup, gesture_num)\n",
    "        #TODO Jennifer: ***ERROR*** total_joint is all zeros for Jaco right now!!! temp placeholder until we get JointMotion to save properly for the Jaco\n",
    "\n",
    "        end_eff, camera, rh, lh, joints = segment_by_demo(total_end_eff, total_camera, total_rh, total_lh, total_joint, demo_max)\n",
    "        # for i in range(0,5):\n",
    "        #     plot_raw_data_subsampled(5, end_eff[i], camera[i], rh[i], lh[i], joint[i])\n",
    "\n",
    "        demo_similarity = np.zeros([4,4,3])\n",
    "\n",
    "        print(\"PID \"+str(singlePIDval)+\", gesture \"+str(gesture_num))\n",
    "        for demo_num1 in range(0,demo_max-1):\n",
    "            for demo_num2 in range(demo_num1+1,demo_max):\n",
    "                # print(\"PID \"+str(singlePIDval)+\", gesture \"+str(gesture_num)+\", demos \"+str(demo_num1)+\",\"+str(demo_num2))\n",
    "                metrics1 = get_evo_metrics(rh[demo_num1],rh[demo_num2])\n",
    "                # metrics2 = get_aligned_evo_metrics(rh[demo_num1],rh[demo_num2])\n",
    "                # print(metrics1)\n",
    "                # print(metrics1,metrics2)\n",
    "\n",
    "                demo_similarity[demo_num1][demo_num2-1][0] = metrics1['rmse']\n",
    "                demo_similarity[demo_num1][demo_num2-1][1] = metrics1['mean']\n",
    "                demo_similarity[demo_num1][demo_num2-1][2] = metrics1['std']\n",
    "\n",
    "        print(demo_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics1['rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
